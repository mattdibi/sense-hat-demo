# Kura AI Wire Component demo

Kura AI Wire Component SenseHat-based demo.

The goal of this repository is to demo the AI inference capabilites of Kura Wires through the use of [NVIDIA Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server). This project trains a [Autencoder](https://en.wikipedia.org/wiki/Autoencoder) in [Tensorflow](https://www.tensorflow.org/) using the data extracted from a Respberry Pi equipped with a [Sense HAT](https://www.raspberrypi.com/products/sense-hat/) with the goal of detecting anomalies in the data retrived by the Sense HAT.

## Project structure

The repository is organized in two main directories:
- **models**: is the Triton server [model repository](https://github.com/triton-inference-server/server/blob/main/docs/model_repository.md) and contains the trained models generated by the training environment.
- **training**: contains the training environment which is comprised of the dataset and the sources for generating the autoencoder model for anomaly detection.

## Training

### Training environment setup

The creation of a Python virtual environment is highly recommended. Create a new environment with the following:

```bash
python3 -m venv .venv
```

Activate it with:

```bash
source .venv/bin/activate
```

Then update `pip` and install the training environment requirements:

```bash
pip3 install --upgrade pip
```

```bash
pip3 install -r training/requirements.txt
```

### Model training

Decompress the datasets

```bash
cd training && unzip *.zip
```

Train the model with the data provided in this repository with:

```bash
./train.py
```

Move the trained model in the Triton model repository and rename it to `model.savedmodel`

```bash
cp -r training/saved_model/autoencoder models/tf_autoencoder_fp32/1
```

```bash
mv models/tf_autoencoder_fp32/1/autoencoder models/tf_autoencoder_fp32/1/model.savedmodel
```

## Inference

For running these models inside Triton, navigate to this repository and run:

```bash
docker run --rm \
    -p4000:8000 \
    -p4001:8001 \
    -p4002:8002 \
    --shm-size=150m \
    -v $(pwd)/models:/models \
    nvcr.io/nvidia/tritonserver:22.01-py3 \
    tritonserver --model-repository=/models --model-control-mode=explicit
```

Excpected models folder structure:

```bash
models
├── postprocessor
│   ├── 1
│   │   └── model.py
│   └── config.pbtxt
├── preprocessor
│   ├── 1
│   │   └── model.py
│   └── config.pbtxt
└── tf_autoencoder_fp32
    ├── 1
    │   └── model.savedmodel
    │       ├── assets
    │       ├── keras_metadata.pb
    │       ├── saved_model.pb
    │       └── variables
    │           ├── variables.data-00000-of-00001
    │           └── variables.index
    └── config.pbtxt
```
